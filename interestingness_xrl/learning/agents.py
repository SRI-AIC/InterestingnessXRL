__author__ = 'Pedro Sequeira'
__email__ = 'pedrodbs@gmail.com'

import numpy as np
from os.path import join, exists
from interestingness_xrl.learning import write_table_csv, read_table_csv, write_3d_table_csv, read_3d_table_csv
from interestingness_xrl.learning.explorations import RandomExploration, SoftMaxExploration

TEXT_EXT = '.csv'
BIN_EXT = '.npz'
C_S_TABLE_FILE_NAME = 'c-s-table'
C_SA_TABLE_FILE_NAME = 'c-sa-table'
C_SAS_TABLE_FILE_NAME = 'c-sas-table'
Q_TABLE_FILE_NAME = 'q-table'
DQ_TABLE_FILE_NAME = 'dq-table'
R_SA_TABLE_FILE_NAME = 'r-sa-table'
T_S_TABLE_FILE_NAME = 't-s-table'
T_SA_TABLE_FILE_NAME = 't-sa-table'


class Agent(object):
    """
    Represents the base class for agents that have a capacity to perceive from and act in some environment, and update
    its internal state.
    """

    def __init__(self, num_states, num_actions):
        """
        Creates a new agent according to the given arguments.
        :param int num_states: the number of states of the world (may include internal features) that the agent can perceive.
        :param int num_actions: the number of actions that the agent can perform in the environment.
        """
        self.num_actions = num_actions
        self.num_states = num_states

    def act(self, state):
        """
        Selects an action to be executed, according to the given observation.
        :param int state: the index of the environment's state as perceived by the agent.
        :return int: the index of the action to be executed.
        """
        pass

    def update(self, prev_state, action, reward, next_state):
        """
        Updates the agent's internal state according to the given sample of interaction with the environment.
        :param int prev_state: the index of the previous state.
        :param int action: the index of the action executed by the agent.
        :param float reward: the reward received by the agent.
        :param int next_state: the index of the state to which the environment transitioned to.
        :return:
        """
        pass

    def save(self, output_dir):
        """
        Saves all the relevant information collected / generated by the agent during its interaction with the environment.
        :param str output_dir: the path to the directory in which to save the agent's information.
        :return:
        """
        pass

    def load(self, output_dir):
        """
        Loads all the relevant information collected / generated by the agent during its interaction with the environment.
        :param str output_dir: the path to the directory from which to load the agent's information.
        :return:
        """
        pass


class ModelAgent(Agent):
    """
    Represents an agent that can model its environment, namely it collects statistics about the states it visits, the
    actions it executes, the rewards it receives, etc.
    """

    def __init__(self, num_states, num_actions, collect_stats=True, action_names=None):
        """
        Creates a new agent according to the given arguments.
        :param int num_states: the number of states of the world (may include internal features) that the agent can perceive.
        :param int num_actions: the number of actions that the agent can perform in the environment.
        :param bool collect_stats: whether to collect stats from the interaction with the environment.
        :param list action_names: the names of the actions available to the agent, in the correct order by which they are indexed.
        """
        super().__init__(num_states, num_actions)
        self.action_names = action_names
        self._collect_stats = collect_stats

        self.c_s = None
        """ Contains information on the states counts/visits. """

        self.c_sa = None
        """ Contains information on the state-action pairs counts/visits. """

        self.c_sas = None
        """ Contains information on the state-action-next state transition counts/visits. """

        self.r_sa = None
        """ Contains information on the state-action pairs average received reward. """

        self.t_s = None
        """ Contains information on the last visits to states. """

        self.t_sa = None
        """ Contains information on the last time actions were executed in states. """

        self.t = 0
        """ The number of learning time-steps, i.e. the number of calls to agent.update(). """

        if self._collect_stats:
            self.c_s = np.zeros(num_states, np.uint32)
            self.c_sa = np.zeros((num_states, num_actions), np.uint32)
            self.c_sas = np.zeros((num_states, num_actions, num_states), np.uint32)
            self.r_sa = np.zeros((num_states, num_actions), np.float)
            self.t_s = np.full(num_states, -1, np.int32)
            self.t_sa = np.full((num_states, num_actions), -1, np.int32)

    def update(self, prev_state, action, reward, next_state):
        """
        Updates the statistics collected on the agent's interaction with the environment according to the given sample.
        :param int prev_state: the index of the previous state.
        :param int action: the index of the action executed by the agent.
        :param float reward: the reward received by the agent.
        :param int next_state: the index of the state to which the environment transitioned to.
        :return:
        """
        if not self._collect_stats:
            return

        # update stats
        old_c_sa = float(self.c_sa[prev_state][action])
        self.r_sa[prev_state][action] = (self.r_sa[prev_state][action] * old_c_sa + reward) / (old_c_sa + 1.)
        self.c_s[prev_state] += 1
        self.c_sa[prev_state][action] += 1
        self.c_sas[prev_state][action][next_state] += 1
        self.t_s[prev_state] = self.t
        self.t_sa[prev_state][action] = self.t
        self.t += 1

    def save(self, output_dir):
        if not self._collect_stats:
            return

        # writes each table to a csv and binary files
        write_table_csv(self.c_s, join(output_dir, C_S_TABLE_FILE_NAME) + TEXT_EXT, col_names=['state'])
        np.savez_compressed(join(output_dir, C_S_TABLE_FILE_NAME) + BIN_EXT, a=self.c_s)
        write_table_csv(self.c_sa, join(output_dir, C_SA_TABLE_FILE_NAME) + TEXT_EXT, col_names=self.action_names)
        np.savez_compressed(join(output_dir, C_SA_TABLE_FILE_NAME) + BIN_EXT, a=self.c_sa)
        write_3d_table_csv(self.c_sas, join(output_dir, C_SAS_TABLE_FILE_NAME) + TEXT_EXT,
                           col_names=['state', 'action', 'next-state', 'count'])
        np.savez_compressed(join(output_dir, C_SAS_TABLE_FILE_NAME) + BIN_EXT, a=self.c_sas)
        write_table_csv(self.r_sa, join(output_dir, R_SA_TABLE_FILE_NAME) + TEXT_EXT, col_names=self.action_names)
        np.savez_compressed(join(output_dir, R_SA_TABLE_FILE_NAME) + BIN_EXT, a=self.r_sa)
        write_table_csv(self.t_s, join(output_dir, T_S_TABLE_FILE_NAME) + TEXT_EXT, col_names=['state'])
        np.savez_compressed(join(output_dir, T_S_TABLE_FILE_NAME) + BIN_EXT, a=self.t_s)
        write_table_csv(self.t_sa, join(output_dir, T_SA_TABLE_FILE_NAME) + TEXT_EXT, col_names=self.action_names)
        np.savez_compressed(join(output_dir, T_SA_TABLE_FILE_NAME) + BIN_EXT, a=self.t_sa)

    def load(self, output_dir):
        if not exists(output_dir):
            return

        self.c_s = np.load(join(output_dir, C_S_TABLE_FILE_NAME) + BIN_EXT)['a'] if \
            exists(join(output_dir, C_S_TABLE_FILE_NAME) + BIN_EXT) else \
            read_table_csv(join(output_dir, C_S_TABLE_FILE_NAME) + TEXT_EXT, dtype=np.uint32, has_header=True)
        self.c_sa = np.load(join(output_dir, C_SA_TABLE_FILE_NAME) + BIN_EXT)['a'] if \
            exists(join(output_dir, C_SA_TABLE_FILE_NAME) + BIN_EXT) else \
            read_table_csv(join(output_dir, C_SA_TABLE_FILE_NAME) + TEXT_EXT, dtype=np.uint32, has_header=True)
        if exists(join(output_dir, C_SAS_TABLE_FILE_NAME) + BIN_EXT):
            self.c_sas = np.load(join(output_dir, C_SAS_TABLE_FILE_NAME) + BIN_EXT)['a']
        else:
            read_3d_table_csv(
                self.c_sas, join(output_dir, C_SAS_TABLE_FILE_NAME) + TEXT_EXT, dtype=np.uint32, has_header=True)
        self.r_sa = np.load(join(output_dir, R_SA_TABLE_FILE_NAME) + BIN_EXT)['a'] if \
            exists(join(output_dir, R_SA_TABLE_FILE_NAME) + BIN_EXT) else \
            read_table_csv(join(output_dir, R_SA_TABLE_FILE_NAME) + TEXT_EXT, dtype=np.float, has_header=True)
        self.t_s = np.load(join(output_dir, T_S_TABLE_FILE_NAME) + BIN_EXT)['a'] if \
            exists(join(output_dir, T_S_TABLE_FILE_NAME) + BIN_EXT) else \
            read_table_csv(join(output_dir, T_S_TABLE_FILE_NAME) + TEXT_EXT, dtype=np.int32, has_header=True)
        self.t_sa = np.load(join(output_dir, T_SA_TABLE_FILE_NAME) + BIN_EXT)['a'] if \
            exists(join(output_dir, T_SA_TABLE_FILE_NAME) + BIN_EXT) else \
            read_table_csv(join(output_dir, T_SA_TABLE_FILE_NAME) + TEXT_EXT, dtype=np.int32, has_header=True)
        self.t = np.max(self.t_s)


class RandomAgent(ModelAgent):
    """
    Represents a random agent, i.e., an agent that selects actions at random, independently of the state.
    """

    def __init__(self, num_states, num_actions, collect_stats=True, action_names=None, rng=None):
        """
        Creates a new agent according to the given arguments.
        :param int num_states: the number of states of the world (may include internal features) that the agent can perceive.
        :param int num_actions: the number of actions that the agent can perform in the environment.
        :param bool collect_stats: whether to collect stats from the interaction with the environment.
        :param list action_names: the names of the actions available to the agent, in the correct order by which they are indexed.
        :param np.random.RandomState rng: the random number generator used by the agent to select its actions.
        """
        super().__init__(num_states, num_actions, collect_stats, action_names)
        self.exploration_strategy = RandomExploration(rng)
        self.exploration_strategy.agent = self

    def act(self, state):
        """
        Selects a random action.
        :param int state: the index of the environment's state as perceived by the agent.
        :return int: the index of the action to be executed.
        """
        return self.exploration_strategy.explore(state)


class QValueBasedAgent(ModelAgent):
    """
    Represents an agent that learns an action-value function (Q-function) from the environment.
    """

    def __init__(self, num_states, num_actions, collect_stats=True, action_names=None,
                 learn_rate=.3, discount=.99, initial_q_value=0., exploration_strategy=None):
        """
        Creates a new agent according to the given arguments.
        :param int num_states: the number of states of the world (may include internal features) that the agent can perceive.
        :param int num_actions: the number of actions that the agent can perform in the environment.
        :param bool collect_stats: whether to collect stats from the interaction with the environment.
        :param list action_names: the names of the actions available to the agent, in the correct order by which they are indexed.
        :param float learn_rate: the agent's learning rate (the weight associated to a new sample during learning).
        :param float discount: the discount factor in [0, 1] (how important are the future rewards?).
        :param float initial_q_value: the value used to initialize the q-function (e.g., for optimistic initialization).
        :param ExplorationStrategy exploration_strategy: the action-selection (exploration-exploitation) strategy used by the agent.
        """
        super().__init__(num_states, num_actions, collect_stats, action_names)

        self.learn_rate = learn_rate
        self.discount = discount
        self.initial_q_value = initial_q_value

        # default epsilon-greedy action-selection (for exploration/exploitation)
        self.exploration_strategy = SoftMaxExploration() \
            if exploration_strategy is None else exploration_strategy
        self.exploration_strategy.agent = self

        # initialize q-table and prediction error (delta-q)
        self.q = np.full((num_states, num_actions), initial_q_value)
        self.dq = np.zeros((num_states, num_actions))

    def act(self, state):
        """
        Selects an action to be executed, according to the given observation and the exploration strategy.
        :param int state: the index of the environment's state as perceived by the agent.
        :return int: the index of the action to be executed.
        """
        return self.exploration_strategy.explore(state)

    def save(self, output_dir):
        super().save(output_dir)
        write_table_csv(self.q, join(output_dir, Q_TABLE_FILE_NAME) + TEXT_EXT, col_names=self.action_names)
        np.savez_compressed(join(output_dir, Q_TABLE_FILE_NAME) + BIN_EXT, a=self.q)
        write_table_csv(self.dq, join(output_dir, DQ_TABLE_FILE_NAME) + TEXT_EXT, col_names=self.action_names)
        np.savez_compressed(join(output_dir, DQ_TABLE_FILE_NAME) + BIN_EXT, a=self.dq)

    def load(self, output_dir):
        if not exists(output_dir):
            return
        super().load(output_dir)
        self.q = np.load(join(output_dir, Q_TABLE_FILE_NAME) + BIN_EXT)['a'] if \
            exists(join(output_dir, Q_TABLE_FILE_NAME) + BIN_EXT) else \
            read_table_csv(join(output_dir, Q_TABLE_FILE_NAME), has_header=True)
        self.dq = np.load(join(output_dir, DQ_TABLE_FILE_NAME) + BIN_EXT)['a'] if \
            exists(join(output_dir, DQ_TABLE_FILE_NAME) + BIN_EXT) else \
            read_table_csv(join(output_dir, DQ_TABLE_FILE_NAME), has_header=True)


class QLearningAgent(QValueBasedAgent):
    """
    Represents an agent that learns a policy using the Q-learning algorithm.
    """

    def __init__(self, num_states, num_actions, collect_stats=True, action_names=None,
                 learn_rate=.3, discount=.99, initial_q_value=0., exploration_strategy=None):
        """
        Creates a new agent according to the given arguments.
        :param int num_states: the number of states of the world (may include internal features) that the agent can perceive.
        :param int num_actions: the number of actions that the agent can perform in the environment.
        :param bool collect_stats: whether to collect stats from the interaction with the environment.
        :param list action_names: the names of the actions available to the agent, in the correct order by which they are indexed.
        :param float learn_rate: the agent's learning rate (the weight associated to a new sample during learning).
        :param float discount: the discount factor in [0, 1] (how important are the future rewards?).
        :param float initial_q_value: the value used to initialize the q-function (e.g., for optimistic initialization).
        :param ExplorationStrategy exploration_strategy: the action-selection (exploration-exploitation) strategy to be used during learning.
        """
        super().__init__(
            num_states, num_actions, collect_stats, action_names,
            learn_rate, discount, initial_q_value, exploration_strategy)

    def update(self, prev_state, action, reward, next_state):
        """
        Updates the Q-value function and the environment's model according to the provided interaction sample.
        :param int prev_state: the index of the previous state.
        :param int action: the index of the action executed by the agent.
        :param float reward: the reward received by the agent.
        :param int next_state: the index of the state to which the environment transitioned to.
        :return:
        """
        super().update(prev_state, action, reward, next_state)

        # updates q-table
        old_q = self.q[prev_state][action]
        self.q[prev_state][action] = \
            old_q + self.learn_rate * (reward + self.discount * np.max(self.q[next_state]) - old_q)

        # updates delta-q (avg prediction error)
        c_sa = float(self.c_sa[prev_state][action])
        dq = np.abs(old_q - self.q[prev_state][action])
        self.dq[prev_state][action] = (self.dq[prev_state][action] * (c_sa - 1) + dq) / c_sa
